# Job Search Automation â€“ PHASE 1

## ğŸ“Œ Project Overview

This project is an **AI-driven job search automation tool**.
The idea is to:

1. **Scrape LinkedIn posts and job listings** using Playwright + Chrome.
2. **Process, parse, and filter** the scraped data to keep only relevant job opportunities.
3. **Store the results** in easy-to-use formats (CSV/Excel).
4. In later phases, add **AI ingestion & agent automation** to validate job authenticity, extract insights, and scale the system.

Right now, we are in **Phase 1 (MVP)**.
This means we focus only on the **core scraping â†’ filtering â†’ storage pipeline** (no AI agents yet).

---

## ğŸ“‚ Folder Structure (Phase 1)

```
project-root/
â”‚
â”œâ”€â”€ data/                     # All data generated by the pipeline
â”‚   â”œâ”€â”€ raw/                  # Scraper outputs (manual/auto JSON/NDJSON)
â”‚   â”œâ”€â”€ processed/            # Parsed & filtered results
â”‚   â””â”€â”€ final/                # Final CSV/Excel for demo/sharing
â”‚
â”œâ”€â”€ scripts/                  # Core functionality
â”‚   â”œâ”€â”€ manual_linkedin_scraper.py   # Manual scraper (attach to Chrome, save posts)
â”‚   â”œâ”€â”€ auto_linkedin_scraper.py     # Auto-scrolling scraper (future use)
â”‚   â”œâ”€â”€ keywords.py                  # Hardcoded + optional user-provided keywords
â”‚   â”œâ”€â”€ parser.py                    # Normalizes raw posts into consistent schema
â”‚   â”œâ”€â”€ filter.py                    # Simple rule-based filtering
â”‚   â””â”€â”€ store.py                     # Saves results to CSV/Excel
â”‚
â”œâ”€â”€ main.py                   # Pipeline entrypoint (parse â†’ filter â†’ store)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

---

## âš™ï¸ How it Works (Phase 1)

1. **Scraping (manual for now)**

   - You run the `manual_linkedin_scraper.py`, which attaches to an already running Chrome session (started in remote debugging mode).
   - Posts or job listings are saved into `./data/raw/` (JSON or NDJSON format).

2. **Parsing (`parser.py`)**

   - Reads raw posts, normalizes them into a consistent schema (title, company, location, text, engagement counts, etc.).
   - Detects **keyword matches** from a hardcoded list (or optional user input).
   - Output â†’ `./data/processed/parsed_posts.json`.

3. **Filtering (`filter.py`)**

   - Removes posts that donâ€™t match any keywords.
   - (Optional) Can filter on likes/comments thresholds.
   - Output â†’ `./data/processed/filtered_posts.json`.

4. **Storing (`store.py`)**

   - Saves filtered results into:

     - `./data/final/filtered_posts.csv`
     - `./data/final/filtered_posts.xlsx`

5. **Main pipeline (`main.py`)**

   - Runs parser â†’ filter â†’ store in one go.
   - Default keywords are used unless you pass `--ask` to provide custom ones.

---

## â–¶ï¸ How to Run (Phase 1)

### Step 1: Launch Chrome with Remote Debugging

```bash
"C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir=C:\temp\chrome-debug
```

Keep this Chrome window open and log in to LinkedIn manually.

### Step 2: Scrape LinkedIn

```bash
python scripts/manual_linkedin_scraper.py
```

This will save posts to `./data/raw/linkedin_posts.json`.

### Step 3: Run the Pipeline

```bash
python main.py
```

or, if you want to provide your own keywords:

```bash
python main.py --ask
```

### Step 4: Check the Results

- Processed posts â†’ `./data/processed/parsed_posts.json`
- Filtered posts â†’ `./data/processed/filtered_posts.json`
- Final outputs â†’ `./data/final/filtered_posts.csv` and `.xlsx`

---

## âœ… Current Achievements (Phase 1)

- Scraping with Playwright (manual, attach to Chrome).
- Parsing raw posts into structured schema.
- Filtering with simple rules (keywords + engagement).
- Exporting to CSV/Excel for sharing.
- Modular pipeline (`main.py`) that ties everything together.

---

## ğŸš€ Next Phase (Phase 2 â€“ Planned)

In **Phase 2**, weâ€™ll move beyond the MVP and add:

1. **Automated scraping**

   - Use the `auto_linkedin_scraper.py` (auto-scroll & extract).
   - No manual intervention required.

2. **Scalability improvements**

   - Handle larger volumes of posts.
   - More robust deduplication & error handling.

3. **AI-powered ingestion**

   - Use LLMs to extract structured job details (title, location, skills, requirements).
   - Validate authenticity by analyzing author profile credibility.

4. **AI Agent Orchestration**

   - Build an orchestrator to run end-to-end automatically: scrape â†’ validate â†’ filter â†’ store â†’ notify.
   - Eventually integrate with notifications (email/Slack).

---

## ğŸ”® Long-Term Vision

The final product will be an **AI job-hunting assistant** that:

- Continuously scrapes job postings.
- Validates authenticity (HR recruiters vs spam).
- Extracts key insights (skills, requirements, salary range).
- Provides real-time alerts to candidates.

---

ğŸ‘‰ This README serves as the **Phase 1 checkpoint**: a working, demo-able MVP pipeline with clear next steps.
