# Job Search Automation – PHASE 1

## 📌 Project Overview

This project is an **AI-driven job search automation tool**.
The idea is to:

1. **Scrape LinkedIn posts and job listings** using Playwright + Chrome.
2. **Process, parse, and filter** the scraped data to keep only relevant job opportunities.
3. **Store the results** in easy-to-use formats (CSV/Excel).
4. In later phases, add **AI ingestion & agent automation** to validate job authenticity, extract insights, and scale the system.

Right now, we are in **Phase 1 (MVP)**.
This means we focus only on the **core scraping → filtering → storage pipeline** (no AI agents yet).

---

## 📂 Folder Structure (Phase 1)

```
project-root/
│
├── data/                     # All data generated by the pipeline
│   ├── raw/                  # Scraper outputs (manual/auto JSON/NDJSON)
│   ├── processed/            # Parsed & filtered results
│   └── final/                # Final CSV/Excel for demo/sharing
│
├── scripts/                  # Core functionality
│   ├── manual_linkedin_scraper.py   # Manual scraper (attach to Chrome, save posts)
│   ├── auto_linkedin_scraper.py     # Auto-scrolling scraper (future use)
│   ├── keywords.py                  # Hardcoded + optional user-provided keywords
│   ├── parser.py                    # Normalizes raw posts into consistent schema
│   ├── filter.py                    # Simple rule-based filtering
│   └── store.py                     # Saves results to CSV/Excel
│
├── main.py                   # Pipeline entrypoint (parse → filter → store)
├── requirements.txt          # Python dependencies
└── README.md                 # This file
```

---

## ⚙️ How it Works (Phase 1)

1. **Scraping (manual for now)**

   - You run the `manual_linkedin_scraper.py`, which attaches to an already running Chrome session (started in remote debugging mode).
   - Posts or job listings are saved into `./data/raw/` (JSON or NDJSON format).

2. **Parsing (`parser.py`)**

   - Reads raw posts, normalizes them into a consistent schema (title, company, location, text, engagement counts, etc.).
   - Detects **keyword matches** from a hardcoded list (or optional user input).
   - Output → `./data/processed/parsed_posts.json`.

3. **Filtering (`filter.py`)**

   - Removes posts that don’t match any keywords.
   - (Optional) Can filter on likes/comments thresholds.
   - Output → `./data/processed/filtered_posts.json`.

4. **Storing (`store.py`)**

   - Saves filtered results into:

     - `./data/final/filtered_posts.csv`
     - `./data/final/filtered_posts.xlsx`

5. **Main pipeline (`main.py`)**

   - Runs parser → filter → store in one go.
   - Default keywords are used unless you pass `--ask` to provide custom ones.

---

## ▶️ How to Run (Phase 1)

### Step 1: Launch Chrome with Remote Debugging

```bash
"C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir=C:\temp\chrome-debug
```

Keep this Chrome window open and log in to LinkedIn manually.

### Step 2: Scrape LinkedIn

```bash
python scripts/manual_linkedin_scraper.py
```

This will save posts to `./data/raw/linkedin_posts.json`.

### Step 3: Run the Pipeline

```bash
python main.py
```

or, if you want to provide your own keywords:

```bash
python main.py --ask
```

### Step 4: Check the Results

- Processed posts → `./data/processed/parsed_posts.json`
- Filtered posts → `./data/processed/filtered_posts.json`
- Final outputs → `./data/final/filtered_posts.csv` and `.xlsx`

---

## ✅ Current Achievements (Phase 1)

- Scraping with Playwright (manual, attach to Chrome).
- Parsing raw posts into structured schema.
- Filtering with simple rules (keywords + engagement).
- Exporting to CSV/Excel for sharing.
- Modular pipeline (`main.py`) that ties everything together.

---

## 🚀 Next Phase (Phase 2 – Planned)

In **Phase 2**, we’ll move beyond the MVP and add:

1. **Automated scraping**

   - Use the `auto_linkedin_scraper.py` (auto-scroll & extract).
   - No manual intervention required.

2. **Scalability improvements**

   - Handle larger volumes of posts.
   - More robust deduplication & error handling.

3. **AI-powered ingestion**

   - Use LLMs to extract structured job details (title, location, skills, requirements).
   - Validate authenticity by analyzing author profile credibility.

4. **AI Agent Orchestration**

   - Build an orchestrator to run end-to-end automatically: scrape → validate → filter → store → notify.
   - Eventually integrate with notifications (email/Slack).

---

## 🔮 Long-Term Vision

The final product will be an **AI job-hunting assistant** that:

- Continuously scrapes job postings.
- Validates authenticity (HR recruiters vs spam).
- Extracts key insights (skills, requirements, salary range).
- Provides real-time alerts to candidates.

---

👉 This README serves as the **Phase 1 checkpoint**: a working, demo-able MVP pipeline with clear next steps.
